{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "451f1f29",
   "metadata": {},
   "source": [
    "## 概要\n",
    "魔搭通过API-Inference，将开源模型服务化并通过API接口进行标准化，让开发者能以更轻量和迅捷的方式体验开源模型，并集成到不同的AI应用中，从而展开富有创造力的尝试，包括与工具结合调用，来构建多种多样的AI应用原型。\n",
    "\n",
    "> [!NOTE]\n",
    "> 欢迎广大开发者[提供反馈](https://modelscope.cn/docs/联系我们)。\n",
    "\n",
    "## 前提条件：创建账号并获取Token\n",
    "API-Inference面向ModelScope注册用户免费提供，请在登陆后获取您专属的访问令牌（Access Token）。具体可以参见[账号注册和登陆](https://modelscope.cn/docs/账号注册与登录)以及[Token的管理](https://modelscope.cn/docs/访问令牌)等相关文档。\n",
    "![img.png](https://resouces.modelscope.cn/document/docdata/2025-7-29_16:10/dist/%E6%A8%A1%E5%9E%8B%E6%9C%8D%E5%8A%A1/%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E6%9C%8D%E5%8A%A1/resources/token.png)\n",
    "## 使用方法\n",
    "\n",
    "### 大语言模型 LLM\n",
    "当前魔搭平台的API-Inference，针对大语言模型提供OpenAI API兼容的接口。 对于LLM模型的API，使用前，请先安装OpenAI SDK:\n",
    "```commandline\n",
    "pip install openai\n",
    "```\n",
    "> [!NOTE]\n",
    "> 其他流行的接口也陆续支持中，例如[Anthropic API](https://docs.anthropic.com/en/api)，可参见下面的 “大语言模型 LLM（Anthropic API兼容接口）” 部分。\n",
    "\n",
    "安装后就可以通过标准的OpenAI调用方式使用。具体调用方式，在每个模型页面右侧的API-Inference范例中以提供，**请以模型页面的 API-Inference 示范代码为准**，尤其例如对于reasoning模型，调用的方式与标准LLM会有一些细微区别。以下范例仅供参考。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9363cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"MODELSCOPE_ACCESS_TOKEN\", # 请替换成您的ModelScope Access Token\n",
    "    base_url=\"https://api-inference.modelscope.cn/v1/\"\n",
    ")\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-Coder-32B-Instruct\", # ModleScope Model-Id\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': 'You are a helpful assistant.'\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': '用python写一下快排'\n",
    "        }\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a55b5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "在这个范例里，使用魔搭的API-Inference，有几个需要适配的有几个地方：\n",
    "- base url: 指向魔搭API-Inference服务 `https://api-inference.modelscope.cn/v1/`。\n",
    "- api_key: 使用魔搭的访问令牌(Access Token), 可以从您的魔搭账号中获取：https://modelscope.cn/my/myaccesstoken 。\n",
    "- 模型名字(model):使用魔搭上开源模型的Model Id，例如`Qwen/Qwen2.5-Coder-32B-Instruct` 。\n",
    "\n",
    "### 大语言模型 LLM（Anthropic API兼容接口）\n",
    "针对LLM模型，API-Inference也支持与Anthropic API兼容的调用方式。要使用Anthropic模式，请在使用前，安装Anthropic SDK:\n",
    "```commandline\n",
    "pip install anthropic\n",
    "```\n",
    "> [!IMPORTANT]\n",
    "> Anthropic API兼容调用方式当前整处于beta测试阶段。如果您在使用过程中遇到任何问题，请联系我们[提供反馈](https://modelscope.cn/docs/联系我们)。\n",
    "\n",
    "安装Anthropic SDK后，即可调用，以下为使用范例。\n",
    "\n",
    "#### 流式调用\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f46c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=\"MODELSCOPE_ACCESS_TOKEN\", # 请替换成您的ModelScope Access Token\n",
    "    base_url=\"https://api-inference.modelscope.cn\")\n",
    "\n",
    "with client.messages.stream(\n",
    "    model=\"Qwen/Qwen2.5-7B-Instruct\", # ModleScope Model-Id\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"write a python quicksort\"}\n",
    "    ],\n",
    "    max_tokens = 1024\n",
    ") as stream:\n",
    "  for text in stream.text_stream:\n",
    "      print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520ad18c",
   "metadata": {},
   "source": [
    "\n",
    "#### 非流式调用\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ecb27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=\"MODELSCOPE_ACCESS_TOKEN\", # 请替换成您的ModelScope Access Token\n",
    "    base_url=\"https://api-inference.modelscope.cn\")\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"Qwen/Qwen2.5-7B-Instruct\", # ModleScope Model-Id\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"write a python quicksort\"}\n",
    "    ],\n",
    "    max_tokens = 1024\n",
    ")\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b469cb9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "在这个范例里，使用魔搭的API-Inference，有几个需要适配的有几个地方：\n",
    "- base url: 指向魔搭API-Inference服务 `https://api-inference.modelscope.cn` 。\n",
    "- api_key: 使用魔搭的访问令牌(Access Token), 可以从您的魔搭账号中获取：https://modelscope.cn/my/myaccesstoken 。\n",
    "- 模型名字(model):使用魔搭上开源模型的Model Id，例如`Qwen/Qwen2.5-Coder-32B-Instruct` 。\n",
    "\n",
    "更多Anthropic API的接口用法以及参数，可以参考 [Anthropic API官方文档](https://docs.anthropic.com/en/api)。\n",
    "\n",
    "### 视觉模型\n",
    "对于视觉VL模型，同样可以通过OpenAI API调用，例如：\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc27d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"MODELSCOPE_ACCESS_TOKEN\", # 请替换成您的ModelScope Access Token\n",
    "    base_url=\"https://api-inference.modelscope.cn/v1\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Qwen/QVQ-72B-Preview\", # ModleScope Model-Id\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.\"}\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/QVQ/demo.png\"}\n",
    "                },\n",
    "                {   \"type\": \"text\", \n",
    "                    \"text\": \"What value should be filled in the blank space?\"\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    stream=True\n",
    "    )\n",
    "\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a740ac",
   "metadata": {},
   "source": [
    "\n",
    "### 文生图模型\n",
    "支持API调用的模型列表，可以通过[AIGC模型](https://www.modelscope.cn/aigc/models)页面进行搜索。\n",
    "API的调用示例如下:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab438b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "endpoint = \"https://api-inference.modelscope.cn/v1/images/generations\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": 'MAILAND/majicflus_v1', # ModelScope Model-Id\n",
    "    \"prompt\": \"A mysterious girl walking down the corridor.\"\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    url = \"https://api-inference.modelscope.cn/v1/images/generations\", \n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer <ModelScope-ACCESS-TOKEN>\", # <ModelScope-ACCESS-TOKEN> 请替换成您的ModelScope 访问令牌\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    },\n",
    "    data = json.dumps(\n",
    "        payload,\n",
    "        ensure_ascii=False).encode('utf-8'),\n",
    ")\n",
    "\n",
    "# get output image\n",
    "image_url = response.json()['images'][0]['url']\n",
    "image = Image.open(BytesIO(requests.get(image_url).content))\n",
    "image.save(\"result_image.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f5bd2",
   "metadata": {},
   "source": [
    "\n",
    "更多参数说明\n",
    "<table>\n",
    "<tr>\n",
    "<td>参数名</td>\n",
    "<td style=\"min-width: 100px;\">参数说明</td>\n",
    "<td style=\"min-width: 100px;\">是否必须</td>\n",
    "<td>参数类型</td>\n",
    "<td>示例</td>\n",
    "<td>取值范围</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>model</td>\n",
    "<td>模型id</td>\n",
    "<td>是</td>\n",
    "<td>string</td>\n",
    "<td>MAILAND/majicflus_v1</td>\n",
    "<td>ModelScope上的AIGC 模型ID</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>prompt</td>\n",
    "<td>正向提示词，大部分模型建议使用英文提示词效果较好。</td>\n",
    "<td>是</td>\n",
    "<td>string</td>\n",
    "<td>A mysterious girl walking down the corridor.</td>\n",
    "<td>长度小于2000</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>negative_prompt</td>\n",
    "<td>负向提示词</td>\n",
    "<td>否</td>\n",
    "<td>string</td>\n",
    "<td>lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry</td>\n",
    "<td>长度小于2000</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>size</td>\n",
    "<td>生成图像分辨率大小</td>\n",
    "<td>否</td>\n",
    "<td>string</td>\n",
    "<td>1024x1024</td>\n",
    "<td>SD系列：[64x64,2048x2048]，FLUX：[64x64,1024x1024]</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>seed</td>\n",
    "<td>随机种子</td>\n",
    "<td>否</td>\n",
    "<td>int</td>\n",
    "<td>12345</td>\n",
    "<td>[0,2^31-1]</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>steps</td>\n",
    "<td>采样步数</td>\n",
    "<td>否</td>\n",
    "<td>int</td>\n",
    "<td>30</td>\n",
    "<td>[1,100]</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>guidance</td>\n",
    "<td>提示词引导系数</td>\n",
    "<td>否</td>\n",
    "<td>float</td>\n",
    "<td>3.5</td>\n",
    "<td>[1.5,20]</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "### 使用限制\n",
    "- 魔搭推理API-Inference，旨在为开发者提供迅速免费的便捷模型调用方式，**请勿用于需要高并发以及SLA保障的线上任务**，如有商业化使用的需求，建议使用各商业化平台的API。\n",
    "- 免费推理API由阿里云提供算力支持，**要求您的ModelScope账号必须[绑定阿里云账号](https://modelscope.cn/docs/阿里云账号绑定与授权教程)后才能正常使用**。\n",
    "- 每位魔搭注册用户，当前每天允许进行**总数为2000次的API-Inference调用**，平台后续可能随时调整此额度。\n",
    "- 实际不同模型允许的调用并发，会根据平台的压力进行动态的速率限制调整，原则上以**保障开发者单并发正常使用**为目标。\n",
    "\n",
    ">[!IMPORTANT] \n",
    "> 出于资源等考虑，平台可能对于部分模型进行单独的限制，这包括单个模型的单天调用总数限制，以及并发限制。\n",
    "> - 例如，[deepseek-ai/DeepSeek-R1-0528](https://www.modelscope.cn/models/deepseek-ai/DeepSeek-R1-0528)当前限制**单模型每天200次**调用额度。其他模型的API调用，可能会有类似的限制并进行动态调整，实际单模型可用次数以及允许的并发，以平台实时调整为准。\n",
    "> - 此外随着新模型的推出，比较早的模型可能逐渐从API-Inference下架，在下架过程中会进一步降低使用额度，直至完全下架。\n",
    "\n",
    "## 支持的模型范围\n",
    "当前API-Inference为魔搭平台上的部分开源**大语言模型（LLM）**，**多模态模型（MLLM）**，以及[**AIGC专区文生图模型**](https://www.modelscope.cn/aigc/models)等，提供了可直接使用的API。\n",
    "\n",
    "API-Inference覆盖的模型范围，主要根据模型在魔搭社区中的关注程度（参考了点赞，下载等数据）来判断。因此，在能力更强，关注度更高的下一代开源模型发布之后，支持的模型清单也会持续迭代。开发者可根据模型页面的过滤条件直接筛选，根据标记有“蓝绿色闪电”的 API-Inference logo 来判断。\n",
    "![img.png](https://resouces.modelscope.cn/document/docdata/2025-7-29_16:10/dist/%E6%A8%A1%E5%9E%8B%E6%9C%8D%E5%8A%A1/%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E6%9C%8D%E5%8A%A1/resources/api-inference-logo.png)\n",
    "\n",
    "同时在模型详情页面右侧，对于支持API-Inference的模型，也会展示使用入口和对应的代码范例。\n",
    "![img.png](https://resouces.modelscope.cn/document/docdata/2025-7-29_16:10/dist/%E6%A8%A1%E5%9E%8B%E6%9C%8D%E5%8A%A1/%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E6%9C%8D%E5%8A%A1/resources/api-inference-sample-code.png)\n",
    "\n",
    "后续我们会积极推进API-Inference支持的模型的覆盖范围，✌️ 敬请期待️。 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
