# backend/app/services/dynamic_controller.py
import json
import logging
from typing import Any, Optional
from sqlalchemy.orm import Session
from app.schemas.chat import ChatRequest, ChatResponse, UserStateSummary, SentimentAnalysisResult
from app.services.sentiment_analysis_service import SentimentAnalysisService
from app.services.user_state_service import UserStateService
from app.services.rag_service import RAGService
from app.services.prompt_generator import PromptGenerator
from app.services.llm_gateway import LLMGateway
from app.services.translation_llm_gateway import translate
from app.services.content_loader import load_json_content  # å¯¼å…¥content_loader
from app.crud.crud_event import event as crud_event
from app.crud.crud_chat_history import chat_history as crud_chat_history
from app.schemas.chat import ChatHistoryCreate
from app.schemas.behavior import BehaviorEvent
# å‡†å¤‡äº‹ä»¶æ•°æ®
from app.schemas.behavior import EventType, AiHelpRequestData
from datetime import datetime
from zoneinfo import ZoneInfo
from typing import AsyncGenerator
logger = logging.getLogger(__name__)
logger = logging.getLogger(__name__)
class DynamicController:
    """åŠ¨æ€æ§åˆ¶å™¨ - ç¼–æ’å„ä¸ªæœåŠ¡çš„æ ¸å¿ƒé€»è¾‘"""

    def __init__(self,
                 user_state_service: UserStateService,
                 sentiment_service: SentimentAnalysisService,
                 rag_service: RAGService,
                 prompt_generator: PromptGenerator,
                 llm_gateway: LLMGateway,
                 clustering_service = None):
        """
        åˆå§‹åŒ–åŠ¨æ€æ§åˆ¶å™¨

        Args:
            user_state_service: ç”¨æˆ·çŠ¶æ€æœåŠ¡
            sentiment_service: æƒ…æ„Ÿåˆ†ææœåŠ¡
            rag_service: RAGæœåŠ¡
            prompt_generator: æç¤ºè¯ç”Ÿæˆå™¨
            llm_gateway: LLMç½‘å…³æœåŠ¡
            clustering_service: èšç±»æœåŠ¡ï¼ˆå¯é€‰ï¼‰
        """
        # éªŒè¯å¿…éœ€çš„æœåŠ¡
        if user_state_service is None:
            raise TypeError("user_state_service cannot be None")
        if prompt_generator is None:
            raise TypeError("prompt_generator cannot be None")
        if llm_gateway is None:
            raise TypeError("llm_gateway cannot be None")
        
        self.user_state_service = user_state_service
        self.sentiment_service = sentiment_service
        self.rag_service = rag_service
        self.prompt_generator = prompt_generator
        self.llm_gateway = llm_gateway
        self.clustering_service = clustering_service
        self.logger = logging.getLogger(__name__)

    async def generate_adaptive_response(
        self,
        request: ChatRequest,
        db: Session,
        background_tasks = None
    ) -> ChatResponse:
        """
        ç”Ÿæˆè‡ªé€‚åº”AIå›å¤çš„æ ¸å¿ƒæµç¨‹

        Args:
            request: èŠå¤©è¯·æ±‚
            db: æ•°æ®åº“ä¼šè¯
            background_tasks: åå°ä»»åŠ¡å¤„ç†å™¨ï¼ˆå¯é€‰ï¼‰

        Returns:
            ChatResponse: AIå›å¤
        """
        logger.info(f"å¼€å§‹è¿›è¡Œç¿»è¯‘...{request.user_message}")
        try:
          
            # æ­¥éª¤1: è·å–æˆ–åˆ›å»ºç”¨æˆ·æ¡£æ¡ˆï¼ˆä½¿ç”¨UserStateServiceï¼‰
            profile, _ = self.user_state_service.get_or_create_profile(request.participant_id, db)
            # æ­¥éª¤2: æƒ…æ„Ÿåˆ†æ
            if self.sentiment_service:
                sentiment_result = self.sentiment_service.analyze_sentiment(
                    request.user_message
                )
            else:
                # å¦‚æœæƒ…æ„Ÿåˆ†ææœåŠ¡æœªå¯ç”¨ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„æƒ…æ„Ÿåˆ†æç»“æœ
                from app.schemas.chat import SentimentAnalysisResult
                sentiment_result = SentimentAnalysisResult(
                    label="neutral",
                    confidence=0.0,
                    details={}
                )

            # æš‚ä¸æ„å»ºç”¨æˆ·çŠ¶æ€æ‘˜è¦ï¼Œç­‰å¾…å†…å®¹åŠ è½½åé€’å¢æé—®è®¡æ•°

            # æ­¥éª¤3: RAGæ£€ç´¢
            retrieved_knowledge = []
            if self.rag_service:
                try:
                    retrieved_knowledge = self.rag_service.retrieve(request.user_message)
                except Exception as e:
                    print(f"âš ï¸ RAGæ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨ç©ºçŸ¥è¯†å†…å®¹: {e}")
                    retrieved_knowledge = []

            # æ­¥éª¤4: åŠ è½½å†…å®¹ï¼ˆå­¦ä¹ å†…å®¹æˆ–æµ‹è¯•ä»»åŠ¡ï¼‰
            content_title = None
            loaded_content_json = None
            if request.mode and request.content_id:
                try:
                    content_type = "learning_content" if request.mode == "learning" else "test_tasks"
                    loaded_content = load_json_content(content_type, request.content_id)
                    content_title = getattr(loaded_content, 'title', None) or getattr(loaded_content, 'topic_id', None)
                    
                    # æ ¹æ®æ¨¡å¼å¤„ç†å†…å®¹
                    # å­¦ä¹ æ¨¡å¼ï¼šæ’é™¤ sc_all å­—æ®µï¼›æµ‹è¯•æ¨¡å¼ï¼šä¿ç•™å®Œæ•´JSON
                    if request.mode == "learning":
                        learning_content_dict = loaded_content.model_dump()
                        learning_content_dict.pop('sc_all', None)
                        loaded_content_json = json.dumps(learning_content_dict, ensure_ascii=False)
                    else:
                        loaded_content_json = loaded_content.model_dump_json()

                except Exception as e:
                    print(f"âš ï¸ å†…å®¹åŠ è½½å¤±è´¥: {e}")
                    loaded_content = None
                    content_title = None
            else:
                loaded_content = None

            # åœ¨ç”Ÿæˆæç¤ºè¯å‰ï¼šé€’å¢æ±‚åŠ©/æé—®è®¡æ•°ï¼Œä½¿å½“å‰è½®æ¬¡å³å¯åæ˜ æœ€æ–°æ¬¡æ•°
            try:
                self.user_state_service.handle_ai_help_request(request.participant_id, content_title)
                # é‡æ–°è·å–æœ€æ–°profileï¼ˆä»Redisï¼‰ï¼Œä»¥åæ˜ é€’å¢åçš„è¡Œä¸ºè®¡æ•°
                profile, _ = self.user_state_service.get_or_create_profile(request.participant_id, db)
            except Exception as _:
                # è®¡æ•°é€’å¢å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
                pass

            # æ­¥éª¤4.5: è¿›åº¦èšç±»åˆ†æï¼ˆåœ¨æ„å»ºç”¨æˆ·çŠ¶æ€æ‘˜è¦å‰ï¼‰
            if request.conversation_history:
                # å°†ConversationMessageè½¬æ¢ä¸ºå­—å…¸æ ¼å¼ç”¨äºèšç±»åˆ†æ
                conversation_for_clustering = []
                for msg in request.conversation_history:
                    conversation_for_clustering.append({
                        'role': msg.role,
                        'content': msg.content
                    })
                
                # ä½¿ç”¨èŠ‚æµé€»è¾‘ï¼šä»…åœ¨æ»¡è¶³æ¡ä»¶æ—¶æ‰è§¦å‘èšç±»åˆ†æ
                should_cluster = self.user_state_service._should_perform_clustering(profile, conversation_for_clustering)
                if should_cluster:
                    try:
                        # è§¦å‘èšç±»åˆ†æï¼šä½¿ç”¨æ³¨å…¥çš„èšç±»æœåŠ¡
                        clustering_result = self.user_state_service.update_progress_clustering(
                            request.participant_id, 
                            conversation_for_clustering,
                            clustering_service=self.clustering_service
                        )
                        
                        if clustering_result and clustering_result.get('analysis_successful'):
                            model_type = clustering_result.get('model_type', 'unknown')
                            print(f"âœ… è·ç¦»èšç±»åˆ†æå®Œæˆ ({model_type}): {clustering_result['cluster_name']} "
                                  f"(ç½®ä¿¡åº¦: {clustering_result.get('confidence', 0):.3f}, ç±»å‹: {clustering_result.get('classification_type', 'unknown')})")
                        
                        # é‡æ–°è·å–profileä»¥åæ˜ èšç±»åˆ†æç»“æœ
                        profile, _ = self.user_state_service.get_or_create_profile(request.participant_id, db)
                        
                    except Exception as e:
                        print(f"âš ï¸ è¿›åº¦èšç±»åˆ†æå¤±è´¥ï¼Œç»§ç»­æ­£å¸¸æµç¨‹: {e}")
                else:
                    print(f"ğŸš¦ èšç±»åˆ†æèŠ‚æµï¼šè·³è¿‡æ­¤æ¬¡è¯·æ±‚ï¼ˆæ¶ˆæ¯æ•°æœªè¾¾åˆ°æ­¥é•¿8æˆ–æ—¶é—´é—´éš”ä¸è¶³ï¼‰")

            # ç°åœ¨æ„å»ºç”¨æˆ·çŠ¶æ€æ‘˜è¦ï¼ˆåŒ…å«æœ€æ–°è¡Œä¸ºè®¡æ•°ã€æƒ…æ„Ÿå’Œèšç±»ç»“æœï¼‰
            user_state_summary = self._build_user_state_summary(profile, sentiment_result)

            # è¯Šæ–­æ—¥å¿—ï¼šè¾“å‡ºæœ¬æ¬¡å¯¹è¯å¯è§çš„BKTå¿«ç…§ä¸ä¸Šä¸‹æ–‡æ³¨å…¥æƒ…å†µ
            try:
                bkt = getattr(user_state_summary, 'bkt_models', {}) or {}
                topic_details = []
                for topic_id, model in (bkt.items() if isinstance(bkt, dict) else []):
                    prob = None
                    if isinstance(model, dict):
                        prob = model.get('mastery_prob')
                    else:
                        prob = getattr(model, 'mastery_prob', None)
                        if prob is None and hasattr(model, 'get_mastery_prob'):
                            try:
                                prob = model.get_mastery_prob()
                            except Exception:
                                prob = None
                    if isinstance(prob, (int, float)):
                        topic_details.append(f"{topic_id}={prob:.3f}")
                    else:
                        topic_details.append(f"{topic_id}=None")
                topic_str = "; ".join(topic_details) if topic_details else "none"
                self.logger.info(
                    f"BKT snapshot for {request.participant_id} (mode={request.mode}, content_id={request.content_id}): "
                    f"topics={len(bkt) if isinstance(bkt, dict) else 0}; {topic_str}"
                )
            except Exception as e:
                self.logger.warning(f"Failed to log BKT snapshot: {e}")

            # æ­¥éª¤5: ç”Ÿæˆæç¤ºè¯
            # å°†ConversationMessageè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            conversation_history_dicts = []
            if request.conversation_history:
                for msg in request.conversation_history:
                    conversation_history_dicts.append({
                        'role': msg.role,
                        'content': msg.content
                    })
            elif request.conversation_history is None:
                # ç¡®ä¿å³ä½¿conversation_historyä¸ºNoneä¹Ÿä¼ é€’ç©ºåˆ—è¡¨
                conversation_history_dicts = []

            retrieved_knowledge_content = [item['content'] for item in retrieved_knowledge if isinstance(item, dict) and 'content' in item]
            # è¯Šæ–­æ—¥å¿—ï¼šä¸Šä¸‹æ–‡æ³¨å…¥è§„æ¨¡
            try:
                test_count = len(request.test_results) if request.test_results else 0
                self.logger.info(
                    f"Context inputs -> RAG={len(retrieved_knowledge_content)}, content_json={'yes' if loaded_content_json else 'no'}, "
                    f"test_results={test_count}"
                )
            except Exception:
                pass
            system_prompt, messages, context_snapshot = self.prompt_generator.create_prompts(
                user_state=user_state_summary,
                retrieved_context=retrieved_knowledge_content,
                conversation_history=conversation_history_dicts,
                user_message=request.user_message,
                code_content=request.code_context,
                mode=request.mode,
                content_title=content_title,
                content_json=loaded_content_json,  # ä¼ é€’åŠ è½½çš„å†…å®¹JSON
                test_results=request.test_results  # ä¼ é€’æµ‹è¯•ç»“æœ
            )

            # æ­¥éª¤6: è°ƒç”¨LLM
            #TODO:doneè¡¨ç¤ºæµå¼è¾“å‡ºæ˜¯å¦å®Œæˆ    elapsed:è¡¨ç¤ºå½“å‰å·²ç»è¾“å‡ºå¤šå°‘å­—
            ai_response = await self.llm_gateway.get_completion(
                system_prompt=system_prompt,
                messages=messages
            )
            # æ­¥éª¤7: æ„å»ºå“åº”ï¼ˆåªåŒ…å«AIå›å¤å†…å®¹ï¼Œç¬¦åˆTDD-II-10è®¾è®¡ï¼‰
            response = ChatResponse(ai_response=ai_response)

            # æ­¥éª¤8: è®°å½•AIäº¤äº’
            self._log_ai_interaction(request, response, db, sentiment_result, background_tasks, system_prompt, content_title, context_snapshot)

            return response

        except Exception as e:
            print(f"âŒ CRITICAL ERROR in generate_adaptive_response: {e}")
            import traceback
            traceback.print_exc()
            # è¿”å›ä¸€ä¸ªæ ‡å‡†çš„ã€ç”¨æˆ·å‹å¥½çš„é”™è¯¯å“åº”
            # ä¸åŒ…å«ä»»ä½•å¯èƒ½æ³„éœ²å†…éƒ¨å®ç°çš„ç»†èŠ‚
            return ChatResponse(
                ai_response="I'm sorry, but a critical error occurred on our end. Please notify the research staff."
            )

    @staticmethod
    def _build_user_state_summary(
        profile: Any,
        sentiment_result: SentimentAnalysisResult
    ) -> UserStateSummary:
        """æ„å»ºç”¨æˆ·çŠ¶æ€æ‘˜è¦"""
        # StudentProfile å·²ç»åŒ…å«äº†æ‰€æœ‰éœ€è¦çš„çŠ¶æ€
        # ä½¿ç”¨ä¼ å…¥çš„sentiment_resultæ¥æ›´æ–°emotion_state
        emotion_state = profile.emotion_state if profile.emotion_state else {}

        # ä½¿ç”¨æƒ…æ„Ÿåˆ†æç»“æœæ›´æ–°emotion_state
        emotion_state["current_sentiment"] = sentiment_result.label
        emotion_state["confidence"] = sentiment_result.confidence
        if sentiment_result.details:
            emotion_state["details"] = sentiment_result.details
        elif "details" not in emotion_state:
            emotion_state["details"] = {}

        # æ›´æ–°ä¼ å…¥çš„profileå¯¹è±¡ä¸­çš„æƒ…æ„ŸçŠ¶æ€
        # ä¿®å¤ï¼šç¡®ä¿åœ¨profile.emotion_stateä¸ºNoneæ—¶ä¸ä¼šå‡ºé”™
        if hasattr(profile, 'emotion_state') and profile.emotion_state is not None:
            profile.emotion_state.update(emotion_state)
        elif hasattr(profile, 'emotion_state') and profile.emotion_state is None:
            # å¦‚æœemotion_stateä¸ºNoneï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„å­—å…¸
            profile.emotion_state = emotion_state

        # behavior_patterns æ›¿ä»£äº† behavior_counters
        behavior_counters = profile.behavior_patterns if hasattr(profile, 'behavior_patterns') else {}
        
        return UserStateSummary(
            participant_id=profile.participant_id,
            emotion_state=emotion_state,
            behavior_counters=behavior_counters,
            behavior_patterns=behavior_counters,
            bkt_models=profile.bkt_model,
            is_new_user=profile.is_new_user,
        )

    def _log_ai_interaction(
        self,
        request: ChatRequest,
        response: ChatResponse,
        db: Session,
        emotion: SentimentAnalysisResult,
        background_tasks: Optional[Any] = None,
        system_prompt: Optional[str] = None,
        content_title: Optional[str] = None,
        context_snapshot: Optional[str] = None,
    ):
        """
        æ ¹æ®TDD-Iè§„èŒƒï¼Œå¼‚æ­¥è®°å½•AIäº¤äº’ã€‚
        1. åœ¨ event_logs ä¸­è®°å½•ä¸€ä¸ª "ai_chat" äº‹ä»¶ã€‚
        2. åœ¨ chat_history ä¸­è®°å½•ç”¨æˆ·å’ŒAIçš„å®Œæ•´æ¶ˆæ¯ã€‚
        3. æ›´æ–°ç”¨æˆ·çŠ¶æ€ä¸­çš„æé—®è®¡æ•°å™¨ï¼ˆå·²åœ¨ç”Ÿæˆæç¤ºè¯å‰å®Œæˆï¼Œä¸åœ¨æ­¤é‡å¤ï¼‰ã€‚
        """
        try:
            # å‡†å¤‡äº‹ä»¶æ•°æ®
            event = BehaviorEvent(
                participant_id=request.participant_id,
                event_type=EventType.AI_HELP_REQUEST,
                event_data=AiHelpRequestData(message=request.user_message).model_dump(),
                # ç»Ÿä¸€ä½¿ç”¨ä¸Šæµ·æ—¶åŒº
                timestamp=datetime.now(ZoneInfo("Asia/Shanghai"))
            )

            # å‡†å¤‡ç”¨æˆ·èŠå¤©è®°å½•
            user_chat = ChatHistoryCreate(
                participant_id=request.participant_id,
                role="user",
                message=request.user_message
            )

            # å‡†å¤‡AIèŠå¤©è®°å½•
            usage = getattr(self.llm_gateway, 'last_usage', None)
            ai_chat = ChatHistoryCreate(
                participant_id=request.participant_id,
                role="assistant",
                message=response.ai_response,
                raw_prompt_to_llm=system_prompt,
                raw_context_to_llm=context_snapshot + json.dumps(emotion.model_dump(), ensure_ascii=False) if context_snapshot else json.dumps(emotion.model_dump(), ensure_ascii=False),
                prompt_tokens=(usage or {}).get('prompt_tokens') if usage else None,
                completion_tokens=(usage or {}).get('completion_tokens') if usage else None,
                total_tokens=(usage or {}).get('total_tokens') if usage else None,
            )

            # æ£€æŸ¥æ˜¯å¦åœ¨Celery Workerç¯å¢ƒä¸­è¿è¡Œï¼ˆbackground_tasksä¸ºNoneï¼‰
            if background_tasks is None:
                # åœ¨Celery Workerä¸­ï¼Œå°†æ•°æ®åº“å†™å…¥æ“ä½œä½œä¸ºç‹¬ç«‹ä»»åŠ¡åˆ†æ´¾åˆ°db_writer_queue
                from app.celery_app import celery_app
                
                # åˆ†æ´¾äº‹ä»¶è®°å½•ä»»åŠ¡
                celery_app.send_task(
                    'app.tasks.db_tasks.log_ai_event_task',
                    args=[event.model_dump()], 
                    queue='db_writer_queue'
                )
                
                # åˆ†æ´¾ç”¨æˆ·æ¶ˆæ¯è®°å½•ä»»åŠ¡
                celery_app.send_task(
                    'app.tasks.db_tasks.save_chat_message_task',
                    args=[user_chat.model_dump()], 
                    queue='db_writer_queue'
                )
                
                # åˆ†æ´¾AIæ¶ˆæ¯è®°å½•ä»»åŠ¡
                celery_app.send_task(
                    'app.tasks.db_tasks.save_chat_message_task',
                    args=[ai_chat.model_dump()], 
                    queue='db_writer_queue'
                )
                
                print(f"INFO: AI interaction for {request.participant_id} queued for async DB write.")
            else:
                # åœ¨FastAPIåº”ç”¨ä¸­ï¼Œä½¿ç”¨FastAPIçš„BackgroundTasksè¿›è¡Œå¼‚æ­¥å¤„ç†
                background_tasks.add_task(crud_event.create_from_behavior, db=db, obj_in=event)
                background_tasks.add_task(crud_chat_history.create, db=db, obj_in=user_chat)
                background_tasks.add_task(crud_chat_history.create, db=db, obj_in=ai_chat)
                print(f"INFO: AI interaction for {request.participant_id} logged asynchronously via FastAPI.")

        except Exception as e:
            # æ•°æ®ä¿å­˜å¤±è´¥å¿…é¡»æŠ¥é”™ï¼Œç§‘ç ”æ•°æ®å®Œæ•´æ€§ä¼˜å…ˆ
            raise RuntimeError(f"Failed to log AI interaction for {request.participant_id}: {e}")

    def generate_adaptive_response_sync(
        self,
        request: ChatRequest,
        db: Session,
        background_tasks = None
    ):
        """
        åŒæ­¥ç”Ÿæˆè‡ªé€‚åº”AIå›å¤çš„æ ¸å¿ƒæµç¨‹ï¼ˆä¾›Celeryä»»åŠ¡ä½¿ç”¨ï¼‰
        Args:
            request: èŠå¤©è¯·æ±‚
            db: æ•°æ®åº“ä¼šè¯
            background_tasks: åå°ä»»åŠ¡å¤„ç†å™¨ï¼ˆå¯é€‰ï¼‰
        Returns:
            ChatResponse: AIå›å¤
        """
        
        try:
            # åˆ›å»ºç¿»è¯‘ç¼“å­˜é¿å…é‡å¤ç¿»è¯‘
            translation_cache = {}
            
            def get_translation(text):
                """è·å–ç¿»è¯‘ç»“æœï¼Œä½¿ç”¨ç¼“å­˜é¿å…é‡å¤ç¿»è¯‘"""
                if text not in translation_cache:
                    translation_cache[text] = translate(text)
                return translation_cache[text]
            
            logger.info(f"ç¿»è¯‘å‰ï¼š{request.user_message}")
            translated_message = get_translation(request.user_message)
            logger.info(f"ç¿»è¯‘åï¼š{translated_message}")
            # æ­¥éª¤1: è·å–æˆ–åˆ›å»ºç”¨æˆ·æ¡£æ¡ˆï¼ˆä½¿ç”¨UserStateServiceï¼‰
            profile, _ = self.user_state_service.get_or_create_profile(request.participant_id, db)
            # æ­¥éª¤2: æƒ…æ„Ÿåˆ†æ
            if self.sentiment_service:
                sentiment_result = self.sentiment_service.analyze_sentiment(
                    translated_message
                )
            else:
                # å¦‚æœæƒ…æ„Ÿåˆ†ææœåŠ¡æœªå¯ç”¨ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„æƒ…æ„Ÿåˆ†æç»“æœ
                from app.schemas.chat import SentimentAnalysisResult
                sentiment_result = SentimentAnalysisResult(
                    label="neutral",
                    confidence=0.0,
                    details={}
                )
            # æš‚ä¸æ„å»ºç”¨æˆ·çŠ¶æ€æ‘˜è¦ï¼Œç­‰å¾…å†…å®¹åŠ è½½åé€’å¢æé—®è®¡æ•°
            # æ­¥éª¤3: RAGæ£€ç´¢
            retrieved_knowledge = []
            if self.rag_service:
                try:
                    retrieved_knowledge = self.rag_service.retrieve(request.user_message)
                except Exception as e:
                    print(f"âš ï¸ RAGæ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨ç©ºçŸ¥è¯†å†…å®¹: {e}")
                    retrieved_knowledge = []
            # æ­¥éª¤4: åŠ è½½å†…å®¹ï¼ˆå­¦ä¹ å†…å®¹æˆ–æµ‹è¯•ä»»åŠ¡ï¼‰
            content_title = None
            loaded_content_json = None
            if request.mode and request.content_id:
                try:
                    content_type = "learning_content" if request.mode == "learning" else "test_tasks"
                    loaded_content = load_json_content(content_type, request.content_id)
                    content_title = getattr(loaded_content, 'title', None) or getattr(loaded_content, 'topic_id', None)
                    
                    # æ ¹æ®æ¨¡å¼å¤„ç†å†…å®¹
                    # å­¦ä¹ æ¨¡å¼ï¼šæ’é™¤ sc_all å­—æ®µï¼›æµ‹è¯•æ¨¡å¼ï¼šä¿ç•™å®Œæ•´JSON
                    if request.mode == "learning":
                        learning_content_dict = loaded_content.model_dump()
                        learning_content_dict.pop('sc_all', None)
                        loaded_content_json = json.dumps(learning_content_dict, ensure_ascii=False)
                    else:
                        loaded_content_json = loaded_content.model_dump_json()
                except Exception as e:
                    print(f"âš ï¸ å†…å®¹åŠ è½½å¤±è´¥: {e}")
                    loaded_content = None
                    content_title = None
            else:
                loaded_content = None
            # åœ¨ç”Ÿæˆæç¤ºè¯å‰ï¼šé€’å¢æ±‚åŠ©/æé—®è®¡æ•°ï¼Œä½¿å½“å‰è½®æ¬¡å³å¯åæ˜ æœ€æ–°æ¬¡æ•°
            try:
                self.user_state_service.handle_ai_help_request(request.participant_id, content_title)
                # é‡æ–°è·å–æœ€æ–°profileï¼ˆä»Redisï¼‰ï¼Œä»¥åæ˜ é€’å¢åçš„è¡Œä¸ºè®¡æ•°
                profile, _ = self.user_state_service.get_or_create_profile(request.participant_id, db)
            except Exception as _:
                pass

            # æ­¥éª¤4.5: è¿›åº¦èšç±»åˆ†æï¼ˆåœ¨æ„å»ºç”¨æˆ·çŠ¶æ€æ‘˜è¦å‰ï¼‰
            if request.conversation_history:
                # å°†ConversationMessageè½¬æ¢ä¸ºå­—å…¸æ ¼å¼ç”¨äºèšç±»åˆ†æ
                conversation_for_clustering = []
                trans_history = []
                for msg in request.conversation_history:
                    conversation_for_clustering.append({
                        'role': msg.role,
                        'content': msg.content
                    })
                
                
                # ä½¿ç”¨èŠ‚æµé€»è¾‘ï¼šä»…åœ¨æ»¡è¶³æ¡ä»¶æ—¶æ‰è§¦å‘èšç±»åˆ†æ
                should_cluster = self.user_state_service._should_perform_clustering(profile,conversation_for_clustering)
                if should_cluster:
                    try:
                        for msg in request.conversation_history:
                            if msg.role == 'user':
                                trans_history.append({
                                    'role': msg.role,
                                    'content': get_translation(msg.content)
                                })
                                logger.info(f"ç¿»è¯‘å†å²ï¼š{trans_history}")
                        # è§¦å‘èšç±»åˆ†æï¼šä½¿ç”¨æ³¨å…¥çš„èšç±»æœåŠ¡
                        clustering_result = self.user_state_service.update_progress_clustering(
                            request.participant_id, 
                            trans_history,
                            clustering_service=self.clustering_service
                        )
                        
                        if clustering_result and clustering_result.get('analysis_successful'):
                            model_type = clustering_result.get('model_type', 'unknown')
                            logger.info(f"âœ… è·ç¦»èšç±»åˆ†æå®Œæˆ (åŒæ­¥-{model_type}): {clustering_result['cluster_name']} "
                                  f"(ç½®ä¿¡åº¦: {clustering_result.get('confidence', 0):.3f}, ç±»å‹: {clustering_result.get('classification_type', 'unknown')})")
                        
                        # é‡æ–°è·å–profileä»¥åæ˜ èšç±»åˆ†æç»“æœ
                        profile, _ = self.user_state_service.get_or_create_profile(request.participant_id, db)
                        
                    except Exception as e:
                        print(f"âš ï¸ è¿›åº¦èšç±»åˆ†æå¤±è´¥ (åŒæ­¥)ï¼Œç»§ç»­æ­£å¸¸æµç¨‹: {e}")
                else:
                    print(f"ğŸš¦ èšç±»åˆ†æèŠ‚æµ (åŒæ­¥)ï¼šè·³è¿‡æ­¤æ¬¡è¯·æ±‚ï¼ˆæ¶ˆæ¯æ•°æœªè¾¾åˆ°æ­¥é•¿8æˆ–æ—¶é—´é—´éš”ä¸è¶³ï¼‰")

            # ç°åœ¨æ„å»ºç”¨æˆ·çŠ¶æ€æ‘˜è¦ï¼ˆåŒ…å«æœ€æ–°è¡Œä¸ºè®¡æ•°ã€æƒ…æ„Ÿå’Œèšç±»ç»“æœï¼‰
            user_state_summary = self._build_user_state_summary(profile, sentiment_result)

            # æ­¥éª¤5: ç”Ÿæˆæç¤ºè¯
            # å°†ConversationMessageè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            conversation_history_dicts = []
            
            if request.conversation_history:
                for msg in request.conversation_history:
                    conversation_history_dicts.append({
                        'role': msg.role,
                        'content': msg.content
                    })
                    
            elif request.conversation_history is None:
                # ç¡®ä¿å³ä½¿conversation_historyä¸ºNoneä¹Ÿä¼ é€’ç©ºåˆ—è¡¨
                conversation_history_dicts = []
            retrieved_knowledge_content = [item['content'] for item in retrieved_knowledge if isinstance(item, dict) and 'content' in item]
            system_prompt, messages, context_snapshot = self.prompt_generator.create_prompts(
                user_state=user_state_summary,
                retrieved_context=retrieved_knowledge_content,
                conversation_history=conversation_history_dicts,
                user_message=request.user_message,
                code_content=request.code_context,
                mode=request.mode,
                content_title=content_title,
                content_json=loaded_content_json,  # ä¼ é€’åŠ è½½çš„å†…å®¹JSON
                test_results=request.test_results  # ä¼ é€’æµ‹è¯•ç»“æœ
            )
            ai_response=""
            # æ­¥éª¤6: è°ƒç”¨LLMï¼ˆåŒæ­¥æ–¹å¼ï¼‰
            for chunck in self.llm_gateway.get_stream_completion_sync(
                system_prompt=system_prompt,
                messages=messages
            ):
                ai_response += chunck
                yield chunck
            # æ­¥éª¤7: æ„å»ºå“åº”ï¼ˆåªåŒ…å«AIå›å¤å†…å®¹ï¼Œç¬¦åˆTDD-II-10è®¾è®¡ï¼‰
            response = ChatResponse(ai_response=ai_response)
            # æ­¥éª¤8: è®°å½•AIäº¤äº’
            self._log_ai_interaction(request, response, db, sentiment_result, background_tasks, system_prompt, content_title, context_snapshot)
            #return response
        except Exception as e:
            print(f"âŒ CRITICAL ERROR in generate_adaptive_response_sync: {e}")
            import traceback
            traceback.print_exc()
            # è¿”å›ä¸€ä¸ªæ ‡å‡†çš„ã€ç”¨æˆ·å‹å¥½çš„é”™è¯¯å“åº”
            # ä¸åŒ…å«ä»»ä½•å¯èƒ½æ³„éœ²å†…éƒ¨å®ç°çš„ç»†èŠ‚
            return ChatResponse(
                ai_response="I'm sorry, but a critical error occurred on our end. Please notify the research staff."
            )
